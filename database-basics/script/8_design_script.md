皆さん、こんにちは。  
本日は「リレーショナルデータベース（RDB）のスケーラビリティ・可用性・セキュリティ」について、実運用の観点から1時間で解説していきます。

この講義は、エンジニア1〜5年目くらいの方を対象にしており、普段業務でRDBを使っているけれども、  
「スケールの限界ってどこ？」「障害にどう備えるの？」「セキュリティ設計ってどうすればいいの？」  
といった疑問を持っている方に向けた内容です。

---

## 本講義のゴール

1つ目は、スケーラビリティ：システムが大きくなっても耐えられる設計とは何かを理解すること。  
2つ目は、可用性：障害や停止が起きたときに、どうやってサービスを守るかを考えられるようになること。  
3つ目は、セキュリティ：データを安全に扱い、外部・内部の脅威に備えるための設計の基本を知ること。

この3つはそれぞれ独立したテーマではなく、現場では必ずトレードオフになります。  
どれか1つではなく「3つのバランス」を取っていくための視点を持って帰ってください。

---

## 今日の論点：スケーラビリティ・可用性・セキュリティ

それぞれのテーマは以下のように整理できます。

- スケーラビリティ：システムの成長に対応できるか
- 可用性：障害が起きても止まらずに提供し続けられるか
- セキュリティ：データを守りながら適切に使える状態か

例えば、
- スケーラブルでもセキュリティが弱ければ情報漏洩します。
- 可用性が高くてもスケーラビリティがなければ将来ボトルネックになります。
- セキュアでも復旧手段がなければ、障害が致命的になります。

---

## 基本用語の確認

### 垂直スケーリング（スケールアップ）

1台のDBサーバーのスペックを上げて性能を強化する手法。  
設計はシンプルだが、マシン性能には上限がある。

### 水平スケーリング（スケールアウト）

複数台のサーバーに処理やデータを分散する手法。  
大規模化に対応できるが、整合性やデータ分割の難易度が上がる。

---

### RPO（Recovery Point Objective）

障害発生時、「どこまでのデータが戻せればよいか」の指標。  
例：RPOが5分 → 「5分前の状態まで復旧できれば許容」

---

### RTO（Recovery Time Objective）

「障害から何分以内に復旧できなければならないか」という制約時間。  
例：RTOが10分 → 「10分以内にサービスを復旧させる必要がある」

---

### 整合性レベル

- 強整合性：更新直後、すべてのノードが同じデータを持つ
- 最終的整合性：一時的にデータがずれても、最終的には一致する

---

## まとめ

スケーラビリティ・可用性・セキュリティの3つの観点を同時に意識することが重要。  
スケーリングは「まず垂直、それでも限界が来たら水平へ」。  
可用性はRPO/RTOを数値として定義し、関係者と合意することが出発点。

---

## チェックリスト

- 自サービスの RPO / RTO は定義されているか？  
- サービスの SLO は数値で言語化できているか？

# 2.0 パフォーマンス設計の基本

---

## はじめに

スケーラビリティやシャーディングを検討する前に、まずは「1台のDBでどこまで性能を出せるか」を最大化することが重要です。  
本セクションでは、RDBにおけるパフォーマンス設計の基礎について解説します。

---

## レイテンシ予算を意識する

サービスの応答時間には限りがあります。  
たとえば「1リクエスト＝100ms以内」と決めた場合、そのうちDBに使えるのはわずか数十msです。

| 処理内容            | 目安時間 |
|---------------------|----------|
| アプリケーション処理 | 40ms     |
| DBクエリ             | 30ms     |
| ネットワーク         | 20ms     |
| その他バッファ       | 10ms     |

このように、DBには「1クエリ＝数ms〜10ms程度」の速度が求められます。

---

## インデックス設計の原則

インデックスはDBの性能を大きく左右する要素です。  
以下の3点が設計時の基本方針です。

### 選択性（selectivity）

- インデックスは「対象データが絞り込まれる」ほど効果を発揮します。  
- 例：性別のような2値のカラムでは効果が薄く、ユーザーIDなどの一意性が高いカラムは効果的です。

### 複合キーの順序

- インデックスは先頭カラムから順にしか使えません。  
- 例：`(user_id, created_at)` は `user_id` を指定すれば `created_at` にも効きますが、その逆は効きません。

### カバリングインデックス

- `SELECT` の対象カラムすべてがインデックスに含まれていれば、テーブル本体へのアクセスをスキップできます。  
- → I/O削減・高速化につながります。

---

## 典型的なアンチパターン

パフォーマンスを下げる「やりがちな実装ミス」を以下に挙げます。

### N+1クエリ

- ループ内で1件ずつDBアクセスするパターン（例：ユーザー一覧を取得 → 各ユーザーの投稿を個別に取得）  
- → JOINやIN句による一括取得に切り替えることで大幅に高速化できます。

### `%キーワード%` のLIKE検索

- 前方一致・後方一致はインデックスが効かない（フルスキャン）  
- → フルテキスト検索エンジンや、部分一致に強い仕組み（N-gram等）を検討

### SELECT *

- 全カラム取得は不要なI/Oを増やします。  
- → 必要なカラムだけを明示的に指定する

### 不要なJOINの多用

- 結合が増えるほどクエリは重くなる  
- → 本当に必要なJOINか？を見直し、分割取得も検討する

---

## 実行計画と計測

パフォーマンスは「設計」だけでなく「計測→改善」がセットです。

### EXPLAIN / EXPLAIN ANALYZE

- クエリの実行計画、インデックスの利用状況、スキャン方式（Seq Scan / Index Scan）などを確認

### 慢SQL（スロークエリ）の監視

- 各DBにスロークエリログや統計ビューが存在します：

| DB | 機能名・手段                  |
|----|-----------------------------|
| PostgreSQL | `pg_stat_statements`, auto_explain |
| MySQL      | `slow_query_log`, `performance_schema` |

- サードパーティの監視ツール（Datadog、NewRelic、Prometheus + Grafana など）と連携するのも有効です。

---

## まとめ

- **スケーリングは万能薬ではない**。まずは単一ノードの設計とチューニングで基礎体力をつけること。  
- インデックス設計では **選択性・順序・カバリング** の3点が基本。  
- **N+1** や **汎用LIKE検索**、**SELECT \*** などのアンチパターンに注意。  
- パフォーマンス改善は、**プロファイル → 仮説 → 実験 → 計測**のループで進める。

---

## チェックリスト

- 慢SQLを検知・可視化できる仕組みはあるか？  
- 主要テーブルのカーディナリティ（値のばらつき）を把握しているか？  
- 高頻度で使われる画面やバッチ処理に対応したインデックス設計になっているか？

# データ分割：パーティショニング & シャーディング（話す原稿）

ではここからは、データ量やアクセス量が増えたときに必要になる「データ分割」の考え方について説明します。

今回扱うのは2つの代表的な手法です：

- パーティショニング（同じテーブルの中で物理的に分ける）
- シャーディング（データベース単位で分割する）

それぞれ目的や性質が異なるため、まずはパーティショニングから見ていきましょう。

---

## パーティショニングの目的

パーティショニングは、**1つの論理テーブルを複数の物理領域に分割する手法**です。  
イメージとしては、「月ごとに1つのテーブル領域を切り替えていく」ような運用です。

目的は大きく3つあります：

1. **メンテナンス性の向上**  
   古いパーティションだけを個別に削除したり、アーカイブできるため、運用が楽になります。

2. **クエリの高速化**  
   `WHERE created_at >= '2024-07-01'` のような条件があると、必要なパーティションだけをスキャンする「パーティションプルーニング」が効きます。

3. **運用の自動化**  
   1ヶ月ごとに新しいパーティションを作って、3ヶ月以上前のものは削除する、といった**ルール化された運用**がしやすくなります。

---

## パーティションキーの設計と注意点

設計次第では、逆に性能が落ちることもあります。

たとえば、「時間」でパーティショニングして、常に“最新1日分”にしかアクセスしないようなケース。  
このとき最新のパーティションだけが高頻度で読まれると、**1つのパーティションに負荷が集中する（＝ホットパーティション）**状態になります。

このような偏りを避けるには、以下のような対策が有効です：

- 時間とIDの複合キーにする  
- パーティション分割数を調整する  
- 負荷状況を定期的に見直す

---

## シャーディングの目的と設計

次にシャーディングです。

これは、**データベース単位で物理的に分割する方法**です。  
具体的には、データベースを複数のサーバーやインスタンスに分け、  
たとえば `user_id % 4` のようなルールで4つのDBに分散して保存する、という形です。

パーティショニングが「1台のDBの中での分割」だとすれば、  
シャーディングは「そもそもDBを4台に分ける」といったイメージです。

シャーディングのメリットは大きく2つあります：

- スケーラビリティが大きく向上します。**横に台数を増やせる**ためです。
- 各シャードが独立しているため、**並列処理が可能**になります。

ただし、その代償も大きいです。

- **クロスシャードJOINが原則使えません。**
- トランザクション制御が難しくなります。
- シャード数の変更には**データの再配置やダウンタイム**が発生する可能性もあります。

---

## シャードキーの選び方

良いシャードキーとは何か？というのが設計上の大きなポイントです。  
ここでは、シャードキー選定の3つの観点を紹介します。

1. **分布の均一性**  
   データが均等に分散されること。特定のシャードだけに偏ると、結局ボトルネックになります。

2. **局所性**  
   1つのリクエストやユーザー操作が、できるだけ**単一シャードで完結**するようにする。複数シャードをまたぐと、トランザクションやJOINが難しくなります。

3. **再シャードのしやすさ**  
   シャード数を変更したいときに、**全データの再配置が必要になるかどうか**を意識する。  
   再シャード時にダウンタイムが長くならない設計が望ましいです。

---

## ルーティングの考え方

シャーディングを導入すると、どのデータがどのDBに入っているかを知る必要があります。  
これを解決するのが**ルーティング戦略**です。

方法は主に2つあります：

- **アプリケーション側でルートを持つ**  
  - `user_id` のハッシュをアプリが計算し、シャードを選ぶ
- **ミドルウェアやルーターを使う**  
  - アプリは1つの入口に投げるだけで、内部的に振り分けられる

アプリ側は柔軟で高速ですが、実装が複雑になります。  
ミドルウェアは開発が簡単になりますが、仕組みがブラックボックス化しやすくなります。

---

## 図で理解する：時系列パーティションとシャーディング構成

ここで2つの図を紹介します。

まず1つ目は**時系列パーティショニング**の図です。

- たとえば「2024_06」「2024_07」「2024_08」と月ごとに分割されたテーブルが並んでいます。
- クエリでは「2024_08」だけにアクセスして、古いパーティションは定期的に削除します。
- これにより、**スキャン範囲が小さくなり、メンテナンスもしやすくなります。**

次に、**シャーディング構成図**です。

- アプリケーションが `user_id` を `mod 4` で割り、それによって「シャードA〜D」にルーティングします。
- 各シャードは別のデータベースで、同じ構造を持っています。
- このように、アプリ or ミドルウェアで分散先を決定して処理する構成になります。

---

## まとめ

- パーティショニングは、**単一DB内でのメンテナンス性とクエリ効率を上げる方法**。
- シャーディングは、**物理的にDBを分割してスケールアウトする方法**。
- まずはパーティショニングで対応し、限界を感じたらシャーディングを検討するのがセオリー。
- シャードキーの選定が設計成功のカギを握ります。

---

## チェックリスト

- 今のシステムにパーティション or シャーディングは必要か？
- 今採用しているシャードキーを1行で説明できるか？なぜそれなのか？
- 再シャード時のデータ移動戦略やダウンタイム対策は設計に含まれているか？

# レプリケーションと整合性（話す原稿）

ここからは、データベースのレプリケーションと整合性について説明します。  
レプリケーションは可用性や読み込み性能を高めるために使われますが、その仕組みや構成によっては整合性の問題が発生します。  
特に、レプリケーション遅延が原因で古いデータが返る現象は、設計段階から考慮が必要です。

---

## 同期レプリケーションと非同期レプリケーションの違い

まずはレプリケーション方式の違いを押さえましょう。

**同期レプリケーション**は、プライマリがレプリカ全てに書き込みが反映されたことを確認してからクライアントにACK（成功応答）を返します。  
そのため、整合性は非常に高いですが、反映までに時間がかかる場合があります。

一方、**非同期レプリケーション**は、プライマリに書き込みが完了した時点でACKを返します。  
レプリカにはその後にデータが反映されるため高速ですが、その間に遅延が発生します。

多くの商用DBはデフォルトで非同期方式を採用しており、スループットを優先する代わりに、一時的な不整合を許容しています。

---

## レプリケーション遅延の実態

非同期方式では、レプリカとプライマリの間でタイムラグが発生します。  
通常は数百ミリ秒〜数秒程度ですが、負荷が高いときやネットワークの問題時には数十秒に達することもあります。

この遅延は、以下のような監視メトリクスで確認できます。

- PostgreSQL：`replication_lag`（秒単位）
- MySQL：`Seconds_Behind_Master`
- Aurora：`Replica replay lag`

運用では、これらを監視してしきい値を超えたらアラートを出す設定が必須です。

---

## Read-after-Write問題

レプリケーション遅延がもたらす典型的な現象が、**Read-after-Write問題**です。

例えば、ユーザーがプロフィールを更新した直後に詳細画面を開くとします。  
書き込みはプライマリに行われ、成功応答が返りますが、レプリカへの反映はまだ終わっていません。  
もしその画面がレプリカから読み取るようになっていると、古いデータが表示されてしまいます。

このような体験は、ユーザーにとっては「更新できていない」ように見えるため、非常にストレスになります。

---

## 整合性を担保するパターン

こうした問題を防ぐためには、アプリケーション側で整合性確保の工夫を入れる必要があります。

### セッション固定（Read Your Writes）
同じユーザーの書き込みと直後の読み取りは、必ず同じノード（プライマリ）に送る方法です。  
セッションやリクエストコンテキストをキーにしてルーティングします。

### ラグ感知＋プライマリ強制読み
レプリカの遅延を監視し、一定のしきい値を超えている場合は読み取りをプライマリに切り替えます。  
これにより、遅延時の整合性を確保できます。

### 楽観ロック／バージョン管理
整合性が保証されないことを前提に、データにバージョン番号や更新日時を付与し、読み込み時に検証します。  
一致しなければリトライやエラー表示に切り替えます。

### フェイルオーバー時のスプリットブレイン対策
スプリットブレインは、ネットワーク障害などで複数ノードが同時にマスターとして動く状態です。  
これを防ぐために、投票制のマスター選出やフェンシング機構を取り入れます。  
クラウドのマネージドDBでは自動フェイルオーバーが備わっている場合もありますが、その挙動は事前に理解しておく必要があります。

---

## 設計のポイント

整合性とスループットはトレードオフです。  
全ての読み取りを強整合にするとパフォーマンスは落ちますが、重要な処理では整合性を優先するべきです。

たとえば、注文確定画面や残高照会はプライマリから読み取る。  
SNSのタイムラインやランキングなどは最終的整合性でも許容する、というように画面ごとにルールを決めておくことが重要です。

---

## まとめ

- 非同期レプリケーションでは、遅延がある前提で設計する。
- Read-after-Write問題を防ぐため、重要な読み取りはプライマリに寄せる。
- 整合性を担保するパターン（セッション固定、ラグ感知、楽観ロック）を使い分ける。
- フェイルオーバー時のスプリットブレイン対策を設計に含める。

---

## チェックリスト

- どの画面が強整合で、どの画面が最終的整合で良いか明文化しているか？
- レプリカ遅延の監視メトリクスを取得しているか？
- フェイルオーバー時の昇格戦略を定義しているか？

# 可用性と復旧計画（RPO/RTO・バックアップ＆リカバリ）話す原稿

ここからは、障害や災害が発生した際にシステムをどう復旧させるか、  
そしてどこまでデータを戻せるかという「可用性と復旧計画」について説明します。

---

## RPO と RTO の考え方

まずは復旧計画の基本となる **RPO** と **RTO** を押さえましょう。




**RPO（Recovery Point Objective）** は「どの時点までデータを戻せればよいか」を示します。  
例えば RPO が5分なら、「障害発生から5分前までのデータがあれば許容する」という意味になります。  
つまり、データ損失の許容範囲を数値化したものです。

**RTO（Recovery Time Objective）** は「どれだけ早く復旧できなければいけないか」を示します。  
例えば RTO が15分なら、「障害が発生してから15分以内にサービスを再開する必要がある」ということです。  
これはシステム停止時間の許容範囲を数値化したものです。

この二つを図にすると、**障害発生 → RPO時点までのデータを復元 → 復旧完了（RTOまで）** という時間軸になります。  
データ損失と停止時間をどう許容するかを、ビジネスと合意しておくことが何より重要です。

---

## バックアップ戦略

RPOを満たすためには、まず適切なバックアップが必要です。

代表的な方式は次の4つです。

1. **フルバックアップ**  
   データベース全体を取得する方法。復旧は容易だが時間と容量を消費する。
2. **増分バックアップ**  
   前回のバックアップ以降の変更分だけを取得する。容量効率は良いが、復旧時には複数の増分を組み合わせる必要がある。
3. **差分バックアップ**  
   最後のフルバックアップからの差分を毎回取得する。復旧は増分より早いが、バックアップサイズが増えやすい。
4. **トランザクションログ／WAL／バイナリログ**  
   時点復旧（PITR）を可能にするために必須。これがないと「直前まで戻す」ことはできない。

さらに、バックアップは保存場所も重要です。  
同じリージョン内だけでなく、異なるリージョンやオフライン環境に保管しておくと、災害にも備えられます。

---

## リカバリ戦略

次にRTOを満たすためのリカバリ方法を見ていきましょう。

- **Point-in-Time Recovery（PITR）**  
  フルバックアップにログを適用して、障害発生直前までデータを復旧します。  
  最も柔軟ですが、復旧時間はログの適用量に依存します。

- **スナップショット復元**  
  直近のスナップショットから一気に復旧する方法です。  
  速いですが、スナップショット取得時点までしか戻せません。

- **災害対策リージョン切替（DR切替）**  
  待機系のシステムを別リージョンに用意し、障害発生時に切り替えます。  
  コストは高いですが、復旧時間は最短になります。

復旧を早めるには、手順を自動化することが大切です。  
また、年に数回は実際に復旧演習を行い、所要時間を測定することも必要です。

---

## 復旧計画の実践

復旧計画は、設計して終わりではなく、**演習で検証して改善する**ことが欠かせません。  
演習で確認すべきポイントは次の通りです。

- バックアップが最新で取得できているか  
- バックアップデータが破損していないか（リストア検証）  
- 復旧にかかる時間がRTO内に収まっているか  
- 担当者が手順を理解しているか

実際に障害が起きたときに慌てないために、日頃から備えておく必要があります。

---

## まとめ

- RPOは「どこまで戻せるか」  
- RTOは「どれだけ早く戻せるか」  
- バックアップとリカバリはセットで考えること  
- 設計だけでなく、演習で本当に数値を満たせるか確認すること

---

## チェックリスト

- 自分のサービスのRPO/RTOは数値で言えるか？  
- 復旧手順書は整備されているか？  
- PITRやスナップショット復旧の演習を最近行ったか？


# セキュリティ設計：アクセス制御／暗号化／監査ログ（話す原稿）

ここからは、データベースにおけるセキュリティ設計について説明します。  
目的はシンプルで、**「事故を起こさない、起きても追える」状態を作ること**です。  
そのために大切なのが、権限の管理、暗号化、そして監査ログです。

---

## 最小権限の原則

まずは「最小権限の原則」です。  
ユーザーやアプリに与える権限は、必要最小限に絞ります。  
これはセキュリティの世界では最も基本的な考え方です。

具体的には次のようにします。

- **テーブル単位の権限付与**  
  不要なSELECTやUPDATEを禁止する  
- **スキーマ分離**  
  アプリケーションが利用するスキーマと、運用・管理用のスキーマを分ける  
- **アプリ用アカウントと運用用アカウントの分離**  
  開発者や運用者が本番DBにフル権限で直接アクセスするのを避ける  

ポイントは、「どの権限を与えるか」ではなく、**「どの権限を外せるか」**という発想です。

---

## アクセス制御の実装

権限だけでなく、アクセス経路をどう制御するかも重要です。

- **ロール設計**  
  役割ごとにロールを作り、必要な権限をまとめる
- **接続元制限**  
  特定のアプリサーバーやネットワークからしか接続できないようにする
- **IAM連携**  
  クラウド環境ではIAM認証を使い、使い捨ての資格情報でDBに接続する
- **秘密情報管理**  
  DBパスワードや鍵はVaultやKMSで集中管理し、アプリに直接埋め込まない

---

## 暗号化の多層防御

暗号化は一つの仕組みで終わりではなく、複数の層で重ねることが重要です。

1. **通信の暗号化（TLS）**  
   クライアントとDB間の通信は必ずTLSで暗号化します。盗聴や改ざんを防ぎます。

2. **保存時の暗号化（TDE／ディスク暗号）**  
   ディスクやストレージに保存されるデータを暗号化します。  
   万が一ディスクが盗まれても、中身を直接読まれるのを防ぎます。

3. **アプリレイヤのフィールド暗号**  
   特に重要な情報はフィールド単位で暗号化します。  
   ただし検索やインデックスが効かなくなるデメリットもあるため、導入時は必ず検証が必要です。

ここで大事なのは、**フィールド暗号は万能ではない**ということです。  
検索性・運用性とのトレードオフを理解した上で導入を判断します。

---

## 監査ログ

次に「監査ログ」です。  
セキュリティは「起こさないこと」も大切ですが、**「起きても追えること」**が同じくらい重要です。

監査ログには、少なくとも以下を記録します。

- 誰が
- いつ
- どのテーブルにアクセスし
- どのデータを変更したか

また、ログは **改ざんできない場所に保管すること** が大切です。  
例えばDB内に置いたままでは管理者が消せてしまうので、外部のログサーバーやWORMストレージに転送して保存します。  

さらに、ログは「ためるだけ」ではなく、**人が見られる仕組み**にすることが重要です。  
ダッシュボードや検知ルールを用意し、不正アクセスや異常操作を検出できるようにします。

---

## 脅威モデルと運用

セキュリティは一度仕組みを作って終わりではなく、**運用の中で守り続けること**が必要です。

- **鍵のローテーション**  
  定期的に秘密鍵や認証情報を更新する  
- **秘密情報の棚卸し**  
  どこに秘密情報が存在するかを把握し、不要なものは削除する  
- **データマスキング**  
  本番環境では一部のデータをマスキングして表示することで、不要な情報露出を避ける  

---

## まとめ

- **最小権限**：不要な権限を外すことを基本にする  
- **アクセス制御**：ロール、接続元制限、IAM、秘密情報管理を組み合わせる  
- **暗号化**：通信・保存・フィールド単位で多層的に守る  
- **監査ログ**：誰が何をしたかを記録し、改ざんできない形で保持する  
- **運用**：鍵ローテーションや棚卸しで継続的に改善する  

---

## チェックリスト

- 本番DBの接続情報はVault/KMSで管理され、定期的にローテーションされているか？  
- アプリ用と運用用のアカウントは分離され、権限は最小化されているか？  
- 監査ログは「人が見て異常を検知できる運用」になっているか？  

# モニタリング & トラブルシューティング、マイグレーション管理（話す原稿）

最後に、**日々の健全性を保ち、変更を安全に回すための仕組み**について説明します。  
ここでは、モニタリング・トラブルシューティング、そしてマイグレーション管理を取り上げます。

---

## モニタリングの粒度

まずはモニタリングです。  
監視は1つの指標だけでは不十分で、複数の層で観測する必要があります。  

### インフラレベル
CPU、ディスクI/O、ネットワーク帯域といったリソース消費を監視します。  
ここで異常があれば、DBやアプリ以前にインフラがボトルネックです。

### DBレベル
接続数の上限、ロックの発生状況、クエリ遅延、レプリケーション遅延などを監視します。  
これらはアプリケーションに直接影響するため、継続的にチェックが必要です。

### アプリレベル
エラー率やレイテンシのp95/p99を観測します。  
「ユーザー体験に直結するSLOを守れているか」を判断するための指標です。

この3層を組み合わせることで、**どの層で問題が発生しているかを切り分けやすくなる**のがポイントです。

---

## 障害対応フロー

次に障害対応です。  
障害はゼロにはできません。重要なのは、起きたときにどう動くかです。

基本の流れは以下の通りです。

1. **検知**  
   監視アラートやユーザー報告で異常を検知します。
2. **切り分け**  
   これはクエリの問題か、インフラ障害か、アプリの不具合かを特定します。
3. **回避策**  
   サービスへの影響を最小化する一時対応を行います。  
   例えば、フェイルオーバーで待機系に切り替える、レート制限をかけるなどです。
4. **恒久対策**  
   原因を突き止めて修正し、プロブレムレビューで再発防止策を残します。

ここで重要なのは、**対応したことを必ず学びに変えること**です。  
単発の火消しで終わらせず、ドキュメントに反映し、次に同じトラブルが起きないようにします。

---

## マイグレーション管理

最後にマイグレーションです。  
DBスキーマの変更はリスクが高いため、**小さく・段階的に・自動化**することが基本です。

### バージョニング
マイグレーションツール（例：Flyway, Liquibase）を使い、変更をコードとして管理します。  
誰が・いつ・何を変更したかを追跡できるようにすることが大切です。

### 前方互換スキーマ
「expand → migrate → contract」という3ステップで進めます。  
- expand：新しいカラムを追加する  
- migrate：アプリを新スキーマに対応させる  
- contract：不要になったカラムや制約を削除する  

この手順にすることで、リリース中もアプリとDBが食い違わないようにできます。

### ロールバック戦略
万が一のときに変更を戻せるように、影響範囲と手順を事前に設計します。  
特に「戻せない変更」（カラム削除やデータ削除）は慎重に扱う必要があります。

---

## まとめ

- モニタリングは **インフラ・DB・アプリの3層で行う**  
- 障害対応は **検知 → 切り分け → 回避策 → 恒久対策** の流れを定める  
- マイグレーションは **小さく・段階的に・前方互換性を持たせて進める**  
- インシデントや変更の経験は必ず学習に変え、チームで共有する  

---

## チェックリスト

- スキーマ変更は自動テストで検証できているか？  
- expand → migrate → contract の段階移行手順を運用に組み込んでいるか？  
- インシデント対応後にレビューを行い、知見をドキュメント化しているか？  

# まとめ & 質疑（話す原稿）

最後に、本日の講義のまとめをお話しします。

---

## 今日の要点

まず、スケーリングの考え方です。  
データベースの性能を上げるときは、**まずは垂直スケーリング**で1台の限界まで性能を引き出し、それでも足りなければ**水平スケーリング**を検討する、という順序が基本です。

次に、整合性と性能についてです。  
これは常にトレードオフの関係にあります。  
全てを強整合にすると性能が出ませんし、全てを最終的整合にするとユーザー体験を損ねます。  
**「どの処理は強整合が必要で、どこは最終的整合で良いか」**を設計時に明確にしておくことが大切です。

復旧計画では、**RPOとRTOを最初に数値で決めること**が重要です。  
「どこまでデータを戻せるのか」「どれだけ早く復旧しなければならないのか」を曖昧にせず、ビジネス側と合意しておく必要があります。

さらに、監視と演習です。  
監視はインフラ・DB・アプリの各層で行い、**慢SQLの監視やレプリケーション遅延のチェック**を日常的に回すことが欠かせません。  
加えて、バックアップからの復旧演習（リハーサル）を定期的に行い、机上の設計で終わらせないようにしましょう。

そしてセキュリティです。  
**最小権限、暗号化、監査ログ**の三本柱を意識してください。  
「不要な権限を外す」「通信・保存・フィールド単位で暗号化をかける」「誰が何をしたかを追えるログを取る」。  
この仕組みがあることで、事故を起こしにくくし、万一発生しても追跡できます。

---

## 次のアクション

ここで今日の話を聞いて終わりにせず、具体的なアクションにつなげてください。

- まずは、自分のサービスの **RPOとRTO、整合性要件を文章化**してみましょう。  
- 次に、**慢SQLを監視する仕組み**が現状あるかを確認してください。もしなければ導入を検討しましょう。  
- そして、**PITR（時点復旧）のリハーサル**をぜひ今月中に実施してみてください。  
  実際に復旧にどれくらい時間がかかるかを測定することは、机上の検討以上に価値があります。

---

## 質疑応答

以上が本日のまとめです。  
ここからは質疑応答の時間にします。  
ぜひ、今日の内容で疑問に思ったことや、自分のプロジェクトに当てはめたときの相談などを遠慮なく聞いてください。
